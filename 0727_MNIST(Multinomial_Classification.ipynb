{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/youngjinjae/pythontest/blob/main/0727_MNIST(Multinomial_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af2e3bbf",
      "metadata": {
        "id": "af2e3bbf"
      },
      "outputs": [],
      "source": [
        "# 필요한 모듈 import\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler # 정규화\n",
        "# 어제는 accuracy만 구했다.\n",
        "from sklearn.metrics import classification_report\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a6d0bec",
      "metadata": {
        "id": "9a6d0bec",
        "outputId": "4f581fad-f34b-4b0e-fbb4-baafa96733a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.96      0.95      1242\n",
            "           1       0.96      0.97      0.97      1429\n",
            "           2       0.91      0.90      0.90      1276\n",
            "           3       0.90      0.89      0.90      1298\n",
            "           4       0.92      0.92      0.92      1236\n",
            "           5       0.88      0.85      0.87      1119\n",
            "           6       0.93      0.96      0.94      1243\n",
            "           7       0.94      0.93      0.93      1334\n",
            "           8       0.87      0.87      0.87      1204\n",
            "           9       0.88      0.89      0.89      1219\n",
            "\n",
            "    accuracy                           0.92     12600\n",
            "   macro avg       0.91      0.91      0.91     12600\n",
            "weighted avg       0.92      0.92      0.92     12600\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\kosa\\anaconda3\\envs\\data_env\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ],
      "source": [
        "# 1. 먼저 sklearn 구현 멀티노미\n",
        "from sklearn import linear_model\n",
        "\n",
        "# raw data loading\n",
        "df = pd.read_csv('./data/mnist/train.csv')\n",
        "# print(df) 확인해보기\n",
        "\n",
        "# 결측치가 있나요. 확인\n",
        "# print(df.isnull().sum()) # 결측치 이상없다.\n",
        "# 이상치 있나요?\n",
        "# boxplot 확인 # 이상없다\n",
        "# 정규화 작업??\n",
        "# sklearn 이 라이브러리가 알아서 정규화 구현\n",
        "\n",
        "#training data set\n",
        "x_data = df.drop('label',axis=1, inplace=False).values\n",
        "#label 날리고 종속변수 남음 # 열 지우기 1 12345 지움 2차원 null 파일\n",
        "t_data = df['label'].values.reshape(-1,1)\n",
        "# 레이블 열과 null 파일 잡고 2차원\n",
        "\n",
        "# 정규화는 안함. sklearn 정규화 자동화 해줌 // tensorflow는 필요함\n",
        "# 데이터 분리\n",
        "x_data_train, x_data_test, t_data_train, t_data_test = \\\n",
        "train_test_split (x_data,\n",
        "                 t_data,\n",
        "                 test_size=0.3,\n",
        "                 random_state=0)\n",
        "\n",
        "# 모델 생성하기\n",
        "sklearn_model = linear_model.LogisticRegression()\n",
        "\n",
        "# 모델 학습하기\n",
        "sklearn_model.fit(x_data_train,\n",
        "                 t_data_train.ravel()) #1차원으로 처리해주세요\n",
        "\n",
        "# 모델 평가하기 classification_report 이걸 사용\n",
        "# 첫번째 인자는 : t_data 의 정답이  들어감\n",
        "# 평가는 test로 확인\n",
        "\n",
        "# 두번째 인자는 : 우리 모델에서 나온 예측값\n",
        "#\n",
        "print(classification_report(t_data_test,\n",
        "                     sklearn_model.predict(x_data_test)))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e5f7fb6",
      "metadata": {
        "id": "0e5f7fb6",
        "outputId": "a6a15ec3-4056-461a-af53-4e641d191b5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "236/236 [==============================] - 1s 2ms/step - loss: 0.4211 - accuracy: 0.8781 - val_loss: 0.3676 - val_accuracy: 0.8901\n",
            "Epoch 2/100\n",
            "236/236 [==============================] - 0s 1ms/step - loss: 0.2978 - accuracy: 0.9131 - val_loss: 0.3373 - val_accuracy: 0.9019\n",
            "Epoch 3/100\n",
            "236/236 [==============================] - 0s 1ms/step - loss: 0.2775 - accuracy: 0.9212 - val_loss: 0.3129 - val_accuracy: 0.9122\n",
            "Epoch 4/100\n",
            "236/236 [==============================] - 0s 1ms/step - loss: 0.2638 - accuracy: 0.9243 - val_loss: 0.3199 - val_accuracy: 0.9136\n",
            "Epoch 5/100\n",
            "236/236 [==============================] - 0s 1ms/step - loss: 0.2579 - accuracy: 0.9230 - val_loss: 0.3520 - val_accuracy: 0.8998\n",
            "Epoch 6/100\n",
            "236/236 [==============================] - 0s 1ms/step - loss: 0.2504 - accuracy: 0.9269 - val_loss: 0.3286 - val_accuracy: 0.9090\n",
            "Epoch 7/100\n",
            "236/236 [==============================] - 0s 1ms/step - loss: 0.2392 - accuracy: 0.9304 - val_loss: 0.3481 - val_accuracy: 0.9036\n",
            "Epoch 8/100\n",
            "236/236 [==============================] - 0s 1ms/step - loss: 0.2453 - accuracy: 0.9291 - val_loss: 0.3296 - val_accuracy: 0.9136\n",
            "Epoch 9/100\n",
            "236/236 [==============================] - 0s 1ms/step - loss: 0.2303 - accuracy: 0.9320 - val_loss: 0.3505 - val_accuracy: 0.9058\n",
            "Epoch 10/100\n",
            "236/236 [==============================] - 0s 1ms/step - loss: 0.2324 - accuracy: 0.9315 - val_loss: 0.3380 - val_accuracy: 0.9138\n",
            "Epoch 11/100\n",
            "236/236 [==============================] - 0s 1ms/step - loss: 0.2342 - accuracy: 0.9309 - val_loss: 0.3472 - val_accuracy: 0.9095\n",
            "Epoch 12/100\n",
            "236/236 [==============================] - 0s 1ms/step - loss: 0.2249 - accuracy: 0.9334 - val_loss: 0.3538 - val_accuracy: 0.9054\n",
            "Epoch 13/100\n",
            "236/236 [==============================] - 0s 1ms/step - loss: 0.2212 - accuracy: 0.9340 - val_loss: 0.3455 - val_accuracy: 0.9107\n",
            "Epoch 14/100\n",
            "236/236 [==============================] - 0s 1ms/step - loss: 0.2221 - accuracy: 0.9344 - val_loss: 0.3480 - val_accuracy: 0.9122\n",
            "Epoch 15/100\n",
            "236/236 [==============================] - 0s 1ms/step - loss: 0.2132 - accuracy: 0.9378 - val_loss: 0.3433 - val_accuracy: 0.9114\n",
            "Epoch 16/100\n",
            "236/236 [==============================] - 0s 1ms/step - loss: 0.2150 - accuracy: 0.9355 - val_loss: 0.3743 - val_accuracy: 0.9051\n",
            "Epoch 17/100\n",
            "236/236 [==============================] - 0s 1ms/step - loss: 0.2130 - accuracy: 0.9373 - val_loss: 0.3951 - val_accuracy: 0.8980\n",
            "Epoch 18/100\n",
            "236/236 [==============================] - 0s 2ms/step - loss: 0.2102 - accuracy: 0.9378 - val_loss: 0.3649 - val_accuracy: 0.9088\n",
            "Epoch 19/100\n",
            "236/236 [==============================] - 0s 2ms/step - loss: 0.2096 - accuracy: 0.9377 - val_loss: 0.3841 - val_accuracy: 0.9027\n",
            "Epoch 20/100\n",
            "236/236 [==============================] - 0s 1ms/step - loss: 0.2087 - accuracy: 0.9384 - val_loss: 0.3767 - val_accuracy: 0.9049\n",
            "Epoch 21/100\n",
            "236/236 [==============================] - 0s 1ms/step - loss: 0.2048 - accuracy: 0.9381 - val_loss: 0.3642 - val_accuracy: 0.9090\n",
            "Epoch 22/100\n",
            "236/236 [==============================] - 0s 1ms/step - loss: 0.2064 - accuracy: 0.9395 - val_loss: 0.3795 - val_accuracy: 0.9054\n",
            "Epoch 23/100\n",
            "236/236 [==============================] - 0s 1ms/step - loss: 0.2018 - accuracy: 0.9392 - val_loss: 0.3671 - val_accuracy: 0.9092\n",
            "Epoch 24/100\n",
            "236/236 [==============================] - 0s 1ms/step - loss: 0.2019 - accuracy: 0.9398 - val_loss: 0.3722 - val_accuracy: 0.9087\n",
            "Epoch 25/100\n",
            "236/236 [==============================] - 0s 1ms/step - loss: 0.2022 - accuracy: 0.9399 - val_loss: 0.3733 - val_accuracy: 0.9073\n",
            "Epoch 26/100\n",
            "236/236 [==============================] - 0s 1ms/step - loss: 0.1983 - accuracy: 0.9406 - val_loss: 0.3843 - val_accuracy: 0.9066\n",
            "Epoch 27/100\n",
            "236/236 [==============================] - 0s 2ms/step - loss: 0.1985 - accuracy: 0.9396 - val_loss: 0.3888 - val_accuracy: 0.9075\n",
            "Epoch 28/100\n",
            "236/236 [==============================] - 0s 2ms/step - loss: 0.1949 - accuracy: 0.9431 - val_loss: 0.3904 - val_accuracy: 0.9051\n",
            "Epoch 29/100\n",
            "236/236 [==============================] - 0s 1ms/step - loss: 0.1960 - accuracy: 0.9418 - val_loss: 0.3971 - val_accuracy: 0.9046\n",
            "Epoch 30/100\n",
            "236/236 [==============================] - 0s 2ms/step - loss: 0.1962 - accuracy: 0.9410 - val_loss: 0.3933 - val_accuracy: 0.9039\n",
            "Epoch 31/100\n",
            "236/236 [==============================] - 0s 2ms/step - loss: 0.1935 - accuracy: 0.9426 - val_loss: 0.3968 - val_accuracy: 0.9060\n",
            "Epoch 32/100\n",
            "236/236 [==============================] - 0s 1ms/step - loss: 0.1904 - accuracy: 0.9436 - val_loss: 0.3898 - val_accuracy: 0.9082\n",
            "Epoch 33/100\n",
            "236/236 [==============================] - 0s 2ms/step - loss: 0.1898 - accuracy: 0.9435 - val_loss: 0.4024 - val_accuracy: 0.9034\n",
            "Epoch 34/100\n",
            "236/236 [==============================] - 0s 2ms/step - loss: 0.1913 - accuracy: 0.9422 - val_loss: 0.4159 - val_accuracy: 0.9029\n",
            "Epoch 35/100\n",
            "236/236 [==============================] - 0s 2ms/step - loss: 0.1879 - accuracy: 0.9440 - val_loss: 0.4072 - val_accuracy: 0.9034\n",
            "Epoch 36/100\n",
            "236/236 [==============================] - 0s 2ms/step - loss: 0.1866 - accuracy: 0.9441 - val_loss: 0.4164 - val_accuracy: 0.9017\n",
            "Epoch 37/100\n",
            "236/236 [==============================] - 0s 2ms/step - loss: 0.1866 - accuracy: 0.9439 - val_loss: 0.4047 - val_accuracy: 0.9054\n",
            "Epoch 38/100\n",
            "236/236 [==============================] - 0s 2ms/step - loss: 0.1883 - accuracy: 0.9426 - val_loss: 0.4045 - val_accuracy: 0.9053\n",
            "Epoch 39/100\n",
            "236/236 [==============================] - 0s 2ms/step - loss: 0.1857 - accuracy: 0.9444 - val_loss: 0.4160 - val_accuracy: 0.9037\n",
            "Epoch 40/100\n",
            "236/236 [==============================] - 0s 2ms/step - loss: 0.1905 - accuracy: 0.9415 - val_loss: 0.4166 - val_accuracy: 0.9053\n",
            "Epoch 41/100\n",
            "236/236 [==============================] - 0s 2ms/step - loss: 0.1879 - accuracy: 0.9438 - val_loss: 0.4253 - val_accuracy: 0.9009\n",
            "Epoch 42/100\n",
            "236/236 [==============================] - 0s 2ms/step - loss: 0.1870 - accuracy: 0.9437 - val_loss: 0.4383 - val_accuracy: 0.9005\n",
            "Epoch 43/100\n",
            "236/236 [==============================] - 0s 2ms/step - loss: 0.1808 - accuracy: 0.9445 - val_loss: 0.4301 - val_accuracy: 0.9049\n",
            "Epoch 44/100\n",
            "236/236 [==============================] - 0s 1ms/step - loss: 0.1789 - accuracy: 0.9452 - val_loss: 0.4507 - val_accuracy: 0.8961\n",
            "Epoch 45/100\n",
            "236/236 [==============================] - 0s 2ms/step - loss: 0.1832 - accuracy: 0.9443 - val_loss: 0.4473 - val_accuracy: 0.8976\n",
            "Epoch 46/100\n",
            "236/236 [==============================] - 0s 2ms/step - loss: 0.1796 - accuracy: 0.9460 - val_loss: 0.4400 - val_accuracy: 0.8966\n",
            "Epoch 47/100\n",
            "236/236 [==============================] - 0s 1ms/step - loss: 0.1808 - accuracy: 0.9455 - val_loss: 0.4368 - val_accuracy: 0.9022\n",
            "Epoch 48/100\n",
            "236/236 [==============================] - 0s 2ms/step - loss: 0.1798 - accuracy: 0.9454 - val_loss: 0.4317 - val_accuracy: 0.9000\n",
            "Epoch 49/100\n",
            "236/236 [==============================] - 0s 2ms/step - loss: 0.1779 - accuracy: 0.9468 - val_loss: 0.4379 - val_accuracy: 0.9022\n",
            "Epoch 50/100\n",
            "236/236 [==============================] - 0s 2ms/step - loss: 0.1773 - accuracy: 0.9464 - val_loss: 0.4476 - val_accuracy: 0.9015\n",
            "Epoch 51/100\n",
            "236/236 [==============================] - 0s 2ms/step - loss: 0.1804 - accuracy: 0.9450 - val_loss: 0.4406 - val_accuracy: 0.9051\n",
            "Epoch 52/100\n",
            "236/236 [==============================] - 0s 2ms/step - loss: 0.1831 - accuracy: 0.9438 - val_loss: 0.4435 - val_accuracy: 0.9043\n",
            "Epoch 53/100\n",
            "236/236 [==============================] - 0s 2ms/step - loss: 0.1762 - accuracy: 0.9471 - val_loss: 0.4395 - val_accuracy: 0.9031\n",
            "Epoch 54/100\n",
            "236/236 [==============================] - 0s 2ms/step - loss: 0.1768 - accuracy: 0.9480 - val_loss: 0.4663 - val_accuracy: 0.8981\n",
            "Epoch 55/100\n",
            "236/236 [==============================] - 0s 2ms/step - loss: 0.1750 - accuracy: 0.9462 - val_loss: 0.4673 - val_accuracy: 0.8995\n",
            "Epoch 56/100\n",
            "236/236 [==============================] - 0s 2ms/step - loss: 0.1801 - accuracy: 0.9453 - val_loss: 0.4465 - val_accuracy: 0.9014\n",
            "Epoch 57/100\n",
            "236/236 [==============================] - 0s 2ms/step - loss: 0.1774 - accuracy: 0.9460 - val_loss: 0.4636 - val_accuracy: 0.8986\n",
            "Epoch 58/100\n",
            "236/236 [==============================] - 0s 2ms/step - loss: 0.1745 - accuracy: 0.9475 - val_loss: 0.4822 - val_accuracy: 0.8917\n",
            "Epoch 59/100\n",
            "236/236 [==============================] - 0s 2ms/step - loss: 0.1785 - accuracy: 0.9462 - val_loss: 0.4745 - val_accuracy: 0.8983\n",
            "Epoch 60/100\n",
            "236/236 [==============================] - 0s 2ms/step - loss: 0.1731 - accuracy: 0.9467 - val_loss: 0.4522 - val_accuracy: 0.9000\n",
            "Epoch 61/100\n",
            "236/236 [==============================] - 0s 2ms/step - loss: 0.1736 - accuracy: 0.9476 - val_loss: 0.4689 - val_accuracy: 0.8969\n",
            "Epoch 62/100\n",
            "236/236 [==============================] - 0s 2ms/step - loss: 0.1694 - accuracy: 0.9491 - val_loss: 0.4800 - val_accuracy: 0.8952\n",
            "Epoch 63/100\n",
            "236/236 [==============================] - 0s 2ms/step - loss: 0.1749 - accuracy: 0.9477 - val_loss: 0.4754 - val_accuracy: 0.8971\n",
            "Epoch 64/100\n",
            "236/236 [==============================] - 0s 2ms/step - loss: 0.1708 - accuracy: 0.9494 - val_loss: 0.4729 - val_accuracy: 0.8981\n",
            "Epoch 65/100\n",
            "236/236 [==============================] - 0s 2ms/step - loss: 0.1761 - accuracy: 0.9461 - val_loss: 0.4880 - val_accuracy: 0.8947\n",
            "Epoch 66/100\n",
            "236/236 [==============================] - 0s 2ms/step - loss: 0.1709 - accuracy: 0.9472 - val_loss: 0.4884 - val_accuracy: 0.8959\n",
            "Epoch 67/100\n",
            "236/236 [==============================] - 0s 2ms/step - loss: 0.1691 - accuracy: 0.9473 - val_loss: 0.4862 - val_accuracy: 0.8935\n",
            "Epoch 68/100\n",
            "236/236 [==============================] - 0s 2ms/step - loss: 0.1735 - accuracy: 0.9463 - val_loss: 0.4850 - val_accuracy: 0.8966\n",
            "Epoch 69/100\n",
            "236/236 [==============================] - 0s 2ms/step - loss: 0.1701 - accuracy: 0.9475 - val_loss: 0.4755 - val_accuracy: 0.8991\n",
            "Epoch 70/100\n",
            "236/236 [==============================] - 0s 2ms/step - loss: 0.1688 - accuracy: 0.9484 - val_loss: 0.4883 - val_accuracy: 0.8963\n",
            "Epoch 71/100\n",
            "236/236 [==============================] - 0s 2ms/step - loss: 0.1655 - accuracy: 0.9497 - val_loss: 0.4950 - val_accuracy: 0.8954\n",
            "Epoch 72/100\n",
            "236/236 [==============================] - 0s 2ms/step - loss: 0.1665 - accuracy: 0.9496 - val_loss: 0.4827 - val_accuracy: 0.8995\n",
            "Epoch 73/100\n",
            "236/236 [==============================] - 0s 2ms/step - loss: 0.1671 - accuracy: 0.9493 - val_loss: 0.5078 - val_accuracy: 0.8917\n",
            "Epoch 74/100\n",
            "236/236 [==============================] - 0s 2ms/step - loss: 0.1700 - accuracy: 0.9477 - val_loss: 0.4955 - val_accuracy: 0.8952\n",
            "Epoch 75/100\n",
            "236/236 [==============================] - 0s 2ms/step - loss: 0.1694 - accuracy: 0.9480 - val_loss: 0.4952 - val_accuracy: 0.8947\n",
            "Epoch 76/100\n",
            "236/236 [==============================] - 0s 2ms/step - loss: 0.1677 - accuracy: 0.9488 - val_loss: 0.5036 - val_accuracy: 0.8951\n",
            "Epoch 77/100\n",
            "236/236 [==============================] - 0s 2ms/step - loss: 0.1666 - accuracy: 0.9500 - val_loss: 0.4935 - val_accuracy: 0.8947\n",
            "Epoch 78/100\n",
            "236/236 [==============================] - 0s 2ms/step - loss: 0.1687 - accuracy: 0.9497 - val_loss: 0.5121 - val_accuracy: 0.8937\n",
            "Epoch 79/100\n",
            "236/236 [==============================] - 0s 2ms/step - loss: 0.1695 - accuracy: 0.9478 - val_loss: 0.5025 - val_accuracy: 0.8940\n",
            "Epoch 80/100\n",
            "236/236 [==============================] - 0s 2ms/step - loss: 0.1657 - accuracy: 0.9505 - val_loss: 0.5108 - val_accuracy: 0.8940\n",
            "Epoch 81/100\n",
            "236/236 [==============================] - 0s 2ms/step - loss: 0.1620 - accuracy: 0.9514 - val_loss: 0.5015 - val_accuracy: 0.8946\n",
            "Epoch 82/100\n",
            "236/236 [==============================] - 0s 2ms/step - loss: 0.1648 - accuracy: 0.9494 - val_loss: 0.5075 - val_accuracy: 0.8995\n",
            "Epoch 83/100\n",
            "236/236 [==============================] - 0s 2ms/step - loss: 0.1668 - accuracy: 0.9493 - val_loss: 0.5035 - val_accuracy: 0.8969\n",
            "Epoch 84/100\n",
            "236/236 [==============================] - 0s 2ms/step - loss: 0.1620 - accuracy: 0.9513 - val_loss: 0.5048 - val_accuracy: 0.9007\n",
            "Epoch 85/100\n",
            "236/236 [==============================] - 0s 2ms/step - loss: 0.1647 - accuracy: 0.9496 - val_loss: 0.5223 - val_accuracy: 0.8934\n",
            "Epoch 86/100\n",
            "236/236 [==============================] - 0s 2ms/step - loss: 0.1700 - accuracy: 0.9480 - val_loss: 0.5121 - val_accuracy: 0.8978\n",
            "Epoch 87/100\n",
            "236/236 [==============================] - 1s 2ms/step - loss: 0.1615 - accuracy: 0.9517 - val_loss: 0.5087 - val_accuracy: 0.8956\n",
            "Epoch 88/100\n",
            "236/236 [==============================] - 0s 2ms/step - loss: 0.1620 - accuracy: 0.9498 - val_loss: 0.5316 - val_accuracy: 0.8940\n",
            "Epoch 89/100\n",
            "236/236 [==============================] - 0s 2ms/step - loss: 0.1639 - accuracy: 0.9501 - val_loss: 0.5149 - val_accuracy: 0.8964\n",
            "Epoch 90/100\n",
            "236/236 [==============================] - 0s 2ms/step - loss: 0.1623 - accuracy: 0.9494 - val_loss: 0.5184 - val_accuracy: 0.8946\n",
            "Epoch 91/100\n",
            "236/236 [==============================] - 0s 2ms/step - loss: 0.1611 - accuracy: 0.9501 - val_loss: 0.5273 - val_accuracy: 0.8929\n",
            "Epoch 92/100\n",
            "236/236 [==============================] - 0s 2ms/step - loss: 0.1629 - accuracy: 0.9505 - val_loss: 0.5243 - val_accuracy: 0.8940\n",
            "Epoch 93/100\n",
            "236/236 [==============================] - 0s 2ms/step - loss: 0.1607 - accuracy: 0.9505 - val_loss: 0.5187 - val_accuracy: 0.8961\n",
            "Epoch 94/100\n",
            "236/236 [==============================] - 0s 2ms/step - loss: 0.1591 - accuracy: 0.9521 - val_loss: 0.5313 - val_accuracy: 0.8937\n",
            "Epoch 95/100\n",
            "236/236 [==============================] - 0s 2ms/step - loss: 0.1623 - accuracy: 0.9494 - val_loss: 0.5301 - val_accuracy: 0.8974\n",
            "Epoch 96/100\n",
            "236/236 [==============================] - 0s 2ms/step - loss: 0.1599 - accuracy: 0.9517 - val_loss: 0.5174 - val_accuracy: 0.8988\n",
            "Epoch 97/100\n",
            "236/236 [==============================] - 0s 2ms/step - loss: 0.1612 - accuracy: 0.9503 - val_loss: 0.5295 - val_accuracy: 0.8995\n",
            "Epoch 98/100\n",
            "236/236 [==============================] - 0s 2ms/step - loss: 0.1632 - accuracy: 0.9493 - val_loss: 0.5269 - val_accuracy: 0.8952\n",
            "Epoch 99/100\n",
            "236/236 [==============================] - 0s 2ms/step - loss: 0.1626 - accuracy: 0.9495 - val_loss: 0.5364 - val_accuracy: 0.8944\n",
            "Epoch 100/100\n",
            "236/236 [==============================] - 0s 2ms/step - loss: 0.1604 - accuracy: 0.9512 - val_loss: 0.5431 - val_accuracy: 0.8937\n"
          ]
        }
      ],
      "source": [
        "# tensrflow 구현\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Flatten, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# from tensorflow.keras.models import Sequential\n",
        "# from tensorflow.keras.layers import Flatten, Dense\n",
        "# from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# raw data loading\n",
        "df = pd.read_csv('./data/mnist/train.csv')\n",
        "\n",
        "# 독립변수와 종속 변수 분리\n",
        "x_data = df.drop('label',axis=1, inplace=False).values\n",
        "t_data = df['label'].values.reshape(-1,1)\n",
        "\n",
        "# 정규화 진행\n",
        "scaler = MinMaxScaler()\n",
        "# 최대 최소 알아야 사용가능\n",
        "scaler.fit(x_data)\n",
        "x_data_norm = scaler.transform(x_data)\n",
        "\n",
        "# train 과 test 데이터 분리\n",
        "x_data_train_norm, x_data_test_norm, t_data_train, t_data_test = \\\n",
        "train_test_split(x_data_norm,\n",
        "                t_data,\n",
        "                test_size=0.3,\n",
        "                random_state=0)\n",
        "\n",
        "# kears model 생성\n",
        "keras_model = Sequential()\n",
        "\n",
        "# layer 를추가\n",
        "keras_model.add(Flatten(input_shape=(784,)))\n",
        "keras_model.add(Dense(10, activation='softmax')) # class 표현 분류의 갯수\n",
        "# softmax는 다중 분류시 그냥 분류시는 아님\n",
        "\n",
        "# model 설정\n",
        "keras_model.compile(optimizer=Adam(learning_rate=1e-2),\n",
        "                    loss='sparse_categorical_crossentropy',\n",
        "                   metrics=['accuracy']\n",
        "                   )# 평가 metrics\n",
        "# model 학습\n",
        "keras_model_result = keras_model.fit(x_data_train_norm,\n",
        "                                    t_data_train,\n",
        "                                    epochs=100,\n",
        "                                     batch_size=100,\n",
        "                                     verbose=1,\n",
        "                                     validation_split=0.2\n",
        "                                    ) # 몇개 짜를거야 val 0% 평가\n",
        "#89.3% 정도\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5834fc91",
      "metadata": {
        "scrolled": false,
        "id": "5834fc91",
        "outputId": "d2ff9e17-46a4-4d35-e4b2-4b8fa135769b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "394/394 [==============================] - 0s 780us/step - loss: 0.5291 - accuracy: 0.8963\n",
            "[0.5290782451629639, 0.8963491916656494]\n",
            "394/394 [==============================] - 0s 628us/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.95      0.95      1242\n",
            "           1       0.95      0.96      0.95      1429\n",
            "           2       0.92      0.84      0.88      1276\n",
            "           3       0.81      0.90      0.86      1298\n",
            "           4       0.93      0.89      0.91      1236\n",
            "           5       0.90      0.80      0.84      1119\n",
            "           6       0.92      0.93      0.92      1243\n",
            "           7       0.94      0.90      0.92      1334\n",
            "           8       0.81      0.89      0.85      1204\n",
            "           9       0.86      0.88      0.87      1219\n",
            "\n",
            "    accuracy                           0.90     12600\n",
            "   macro avg       0.90      0.89      0.90     12600\n",
            "weighted avg       0.90      0.90      0.90     12600\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 모델 평가\n",
        "print(keras_model.evaluate(x_data_test_norm,t_data_test)) #89.6%\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "print(classification_report(t_data_test,\n",
        " tf.argmax(keras_model.predict(x_data_test_norm),axis=1).numpy()))\n",
        "\n",
        "# 정답 변수, 우리의 예측, 열아니면 행으로 찾아서 predict는 2차원으로\n",
        "# tensorflow 파일 쓰면 안되고 numpy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b85e641",
      "metadata": {
        "id": "1b85e641"
      },
      "outputs": [],
      "source": [
        "# test 데이터 이용해서 예측하고\n",
        "# 모델의 결과를 결과피일로 만들어서 kaggle에 제출하세요.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce3c66b7",
      "metadata": {
        "id": "ce3c66b7"
      },
      "outputs": [],
      "source": [
        "# 이런 비정형 데이터는 어덯게 학습은 좋은가?\n",
        "# 실제로는 데이터값이 따라 안좋음\n",
        "\n",
        "# 가장 쉬운 방법은 layer를 늘리면 되요,\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:data_env] *",
      "language": "python",
      "name": "conda-env-data_env-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.17"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}